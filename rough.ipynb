{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's Make a CNN\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's Make a CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 14419274\n",
      "Total Operations: 254899200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape=(3, 224, 224),\n",
    "                 num_classes=10,\n",
    "                 num_filters=[32, 64, 128, 256, 512],\n",
    "                 kernel_size=3,\n",
    "                 dropout=0.5,\n",
    "                 batch_norm=True,\n",
    "                 conv_activation='ReLU',\n",
    "                 pooling='max',\n",
    "                 pooling_kernel_size=2,\n",
    "                 pooling_stride=2,\n",
    "                 output_activation='softmax',\n",
    "                 dense_neurons=512):\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = input_shape[0]\n",
    "        \n",
    "        for filters in num_filters:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, filters, kernel_size=kernel_size, padding=1))\n",
    "            if batch_norm:\n",
    "                self.conv_layers.append(nn.BatchNorm2d(filters))\n",
    "            self.conv_layers.append(getattr(nn, conv_activation)())\n",
    "            if pooling == 'max':\n",
    "                self.conv_layers.append(nn.MaxPool2d(kernel_size=pooling_kernel_size, stride=pooling_stride))\n",
    "            in_channels = filters\n",
    "        \n",
    "        self.flatten_size = self._get_flatten_size(input_shape)\n",
    "        self.dense_layer = nn.Linear(self.flatten_size, dense_neurons)\n",
    "        self.dense_activation = getattr(nn, conv_activation)()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(dense_neurons, num_classes)\n",
    "        self.output_activation = nn.Softmax(dim=1) if output_activation == 'softmax' else nn.Identity()\n",
    "\n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        x = torch.rand(1, *input_shape)\n",
    "        with torch.no_grad():\n",
    "            for layer in self.conv_layers:\n",
    "                x = layer(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.dense_activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return self.output_activation(x)\n",
    "\n",
    "    def compute_total_params_and_ops(self, input_shape):\n",
    "        total_params = 0\n",
    "        total_ops = 0\n",
    "        h, w = input_shape[1], input_shape[2]\n",
    "        in_channels = input_shape[0]\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                k = layer.kernel_size[0]\n",
    "                out_h, out_w = h // 2, w // 2\n",
    "                total_ops += out_h * out_w * layer.out_channels * in_channels * k * k\n",
    "                total_params += (in_channels * k * k + 1) * layer.out_channels\n",
    "                h, w, in_channels = out_h, out_w, layer.out_channels\n",
    "\n",
    "        total_params += (self.flatten_size + 1) * self.dense_layer.out_features\n",
    "        total_ops += self.flatten_size * self.dense_layer.out_features\n",
    "\n",
    "        total_params += (self.dense_layer.out_features + 1) * self.output_layer.out_features\n",
    "        total_ops += self.dense_layer.out_features * self.output_layer.out_features\n",
    "\n",
    "        return total_params, total_ops\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, learning_rate, epochs, device):\n",
    "        self.to(device)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_loss = running_loss / total\n",
    "            train_acc = correct / total\n",
    "\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion, device)\n",
    "\n",
    "            wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "                       \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "            print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    def validate(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return val_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32, val_split=0.2, augment=True, image_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads the iNaturalist dataset, applies transformations, and splits the training set while maintaining class balance.\n",
    "    Parameters:\n",
    "        data_dir (str): Path to dataset containing 'train' and 'test' folders.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        val_split (float): Fraction of training data to use for validation.\n",
    "        augment (bool): Whether to apply data augmentation to training data.\n",
    "        image_size (tuple): Target image size for resizing.\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "\n",
    "    # Define basic transformations\n",
    "    transform_train = [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomHorizontalFlip() if augment else transforms.Lambda(lambda x: x),\n",
    "        transforms.RandomRotation(15) if augment else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    "\n",
    "    transform_test = [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    "\n",
    "    transform_train = transforms.Compose([t for t in transform_train if not isinstance(t, transforms.Lambda)])\n",
    "    transform_test = transforms.Compose(transform_test)\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=transform_test)\n",
    "\n",
    "    # Stratified split for validation set\n",
    "    train_indices, val_indices = stratified_split(train_dataset, val_split)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def stratified_split(dataset, val_split):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and validation sets with class balance.\n",
    "    \"\"\"\n",
    "    labels = np.array([dataset.targets[i] for i in range(len(dataset))])\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(labels)), test_size=val_split, stratify=labels, random_state=42\n",
    "    )\n",
    "    return train_indices, val_indices\n",
    "\n",
    "# Example usage\n",
    "# data_dir = \"path_to_inat_dataset\"\n",
    "# train_loader, val_loader, test_loader = get_dataloaders(data_dir, augment=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from inat_preprocessing import get_dataloaders\n",
    "from cnn_model import CNN  # Assuming CNN class is in cnn_model.py\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct = 0.0, 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0.0, 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return val_loss / total, correct / total\n",
    "\n",
    "def train_model(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Data Loaders\n",
    "        train_loader, val_loader, _ = get_dataloaders(\n",
    "            data_dir=\"/path/to/inat_dataset\",\n",
    "            batch_size=config.batch_size,\n",
    "            augment=config.augment\n",
    "        )\n",
    "\n",
    "        # Model\n",
    "        model = CNN(\n",
    "            num_filters=config.num_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            dropout=config.dropout,\n",
    "            batch_norm=config.batch_norm,\n",
    "            conv_activation=config.conv_activation\n",
    "        ).to(device, non_blocking=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "                       \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "            print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"batch_size\": {\"values\": [32, 64]},\n",
    "            \"num_filters\": {\"values\": [[32, 64, 128, 256, 512], [64, 128, 256, 512, 1024]]},\n",
    "            \"kernel_size\": {\"values\": [3, 5]},\n",
    "            \"dropout\": {\"values\": [0.2, 0.3]},\n",
    "            \"batch_norm\": {\"values\": [True, False]},\n",
    "            \"conv_activation\": {\"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]},\n",
    "            \"augment\": {\"values\": [True, False]},\n",
    "            \"learning_rate\": {\"values\": [0.001, 0.0005]},\n",
    "            \"epochs\": {\"values\": [10, 20]}\n",
    "        }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"inat-cnn-sweep\")\n",
    "    wandb.agent(sweep_id, train_model, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
